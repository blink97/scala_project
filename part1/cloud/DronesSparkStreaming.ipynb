{"nbformat_minor": 2, "cells": [{"source": "# SCALA SPARK DRONES\n\nThis notebook allow spark operation for the drones project", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 789.97119140625, "end_time": 1559227834662.209}}}}, {"source": "## Packages \n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0,com.google.code.gson:gson:2.4\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1559225971971_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:8088/proxy/application_1559225971971_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1559225971971_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0,com.google.code.gson:gson:2.4', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1559225971971_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:8088/proxy/application_1559225971971_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1559225971971_0003_01_000001/livy\">Link</a></td><td></td></tr><tr><td>2</td><td>application_1559225971971_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:8088/proxy/application_1559225971971_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-spark.jzazxy1ukepebci422s4ptt1zf.zx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1559225971971_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 778.81689453125, "end_time": 1559228005459.343}}, "collapsed": false}}, {"source": "## Kafka Topic\n\n\n\nFrom Bash or other Unix shell:\n\n```bash\nCLUSTERNAME='spark-realscala'\nUSER='admin42\nPASSWORD='RealScalaPass42Epita'\ncurl -u $USER:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2\n```\n\nReplace the YOUR_ZOOKEEPER_HOSTS in the next cell with the returned value, and then run the cell", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 800.447021484375, "end_time": 1559228006270.005}}}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic drones --zookeeper 'YOUR_ZOOKEEPER_HOSTS'", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Setup and Configuration\n\n1. Link to drones data\n2. kafka brokers configs\n\nWe need kafka brokers hosts, we can get them via bash:\n\n```bash\nCLUSTERNAME='spark-realscala'\nUSER='admin42\nPASSWORD='RealScalaPass42Epita'\ncurl -u $USER:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n```\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport java.util.HashMap\nimport com.google.gson.Gson\n\n// Vals for drones data TODO\n// TODO\n// TODO\n// TODO\n\n\n// Kafka configuration\n// kafkaBrokers should contain a comma-delimited list of brokers. For example:\n// kafkaBrokers = \"wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn2-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\"\nval kafkaBrokers=\"your Kafka brokers\"\nval kafkaTopic=\"drones\"\n\nprintln(\"Finished configuring\")", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Start Stream & Streaming\n\n**TODO** : \n* Fix Stream ! MsgToJson, or already as json, etc...", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "// track the number of drones msgs emitted to Kafka\nval numDronesMsg = sc.accumulator(0L, \"Drones Msg sent to Kafka\")\n\ndef createStreamingContext(): StreamingContext = {\n    // Create the Kafka producer\n    val producerProperties = new HashMap[String, Object]()\n    producerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\n    producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\n    producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\n    \n    // set up the streaming context\n    val ssc = new StreamingContext(sc, Seconds(5))\n    // set up the stream, which we just convert to JSON\n    // TODO !!!!\n    val stream = ...\n    // Write the data to Kafka\n    stream.foreachRDD( rdd => {\n        rdd.foreachPartition( partition => {\n            val producer = new KafkaProducer[String, String](producerProperties)\n            partition.foreach( record => {\n                // Convert the data to JSON\n                val gson = new Gson()\n                val data = gson.toJson(record)\n                val message = new ProducerRecord[String, String](kafkaTopic, null, data)\n                \n                producer.send(message)\n                \n                // Increment the counter\n                numDronesMsg +=1\n            })\n            producer.close()\n        })\n    })\n    ssc\n}\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there were two deprecation warnings; re-run with -deprecation for details\naccDronesMsg: org.apache.spark.Accumulator[Long] = 0"}], "metadata": {"cell_status": {"execute_time": {"duration": 1083.236083984375, "end_time": 1559228724816.417}}, "collapsed": false}}, {"source": "Writing to Kafka :", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val ssc = StreamingContext.getActiveOrCreate(createStreamingContext)\nssc.start()\n// Timeout after 60 seconds\nssc.awaitTerminationOrTimeout(60000)\nprintln(\"Finished writting \" + numDronesMsg + \" drones messages to Kafka\")", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}